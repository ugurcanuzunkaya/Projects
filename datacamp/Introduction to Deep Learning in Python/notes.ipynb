{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning in Python Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Deep Learning\n",
    "- Deep learning is a subset of machine learning, which is a subset of artificial intelligence.\n",
    "- Deep learning is a type of machine learning that trains a computer to perform human-like tasks, such as recognizing speech, identifying images or making predictions.\n",
    "- Instead of organizing data to run through predefined equations, deep learning sets up basic parameters about the data and trains the computer to learn on its own by recognizing patterns using many layers of processing.\n",
    "- We use Keras for this course. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "- Forward propagation is the process neural networks use to make predictions.\n",
    "- Bank transactions example:\n",
    "    - Inputs: number of children, number of existing accounts.\n",
    "    - Output: number of transactions.\n",
    "    - Weights: parameters that the model learns.\n",
    "    - [Model Photo](forward_propagation.png)\n",
    "- Dot product: multiply the inputs by the weights and sum them up.\n",
    "- Forward propagation for one data point. Output is the prediction for that data point.\n",
    "- Code:\n",
    "```python\n",
    "import numpy as np\n",
    "input_data = np.array([2, 3])\n",
    "weights = {'node_0': np.array([1, 1]),\n",
    "           'node_1': np.array([-1, 1]),\n",
    "           'output': np.array([2, -1])}\n",
    "node_0_value = (input_data * weights['node_0']).sum()\n",
    "node_1_value = (input_data * weights['node_1']).sum()\n",
    "hidden_layer_values = np.array([node_0_value, node_1_value])\n",
    "print(hidden_layer_values)\n",
    "output = (hidden_layer_values * weights['output']).sum()\n",
    "print(output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "- Activation functions are applied to node inputs to produce node output.\n",
    "- An activation function allows models to capture non-linearities.\n",
    "- If the relationships in the data are non-linear, we need an activation function to capture them.\n",
    "- Applied to node inputs, activation functions produce node outputs.\n",
    "- Activation functions:\n",
    "    - ReLU (Rectified Linear Activation): max(0, x)\n",
    "    - Tanh (Hyperbolic Tangent): (e^x - e^-x) / (e^x + e^-x)\n",
    "    - Sigmoid: 1 / (1 + e^-x)\n",
    "    - Identity: f(x) = x\n",
    "- [Activation Function Tanh](activation_function_tanh.png)\n",
    "\n",
    "### ReLU (Rectified Linear Activation)\n",
    "- ReLU is the most common activation function.\n",
    "- [Activation Function ReLU](ReLU.png)\n",
    "\n",
    "### Tanh Example\n",
    "- Example code:\n",
    "```python\n",
    "import numpy as np\n",
    "input_data = np.array([-1, 2])\n",
    "weights = {'node_0': np.array([3, 3]),\n",
    "           'node_1': np.array([1, 5]),\n",
    "           'output': np.array([2, -1])}\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = np.tanh(node_0_input)\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = np.tanh(node_1_input)\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "output = (hidden_layer_outputs * weights['output']).sum()\n",
    "print(output)\n",
    "```\n",
    "\n",
    "### ReLU Example\n",
    "- Example code:\n",
    "```python\n",
    "def relu(input):\n",
    "    '''Define your relu activation function here'''\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(0, input)\n",
    "    \n",
    "    # Return the value just calculated\n",
    "    return(output)\n",
    "\n",
    "# Calculate node 0 value: node_0_output\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = relu(node_0_input)\n",
    "\n",
    "# Calculate node 1 value: node_1_output\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = relu(node_1_input)\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "# Calculate model output (do not apply relu)\n",
    "model_output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print model output\n",
    "print(model_output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper Networks\n",
    "- There is more than one hidden layer. \n",
    "- Deep networks internally build representations of patterns in the data.\n",
    "- You use same forward propagation process, but apply it multiple times iteratively.\n",
    "- [Deeper Networks](multiple_hidden_layers_relu.png)\n",
    "- This is the mechanics for how neural networks make predictions.\n",
    "\n",
    "### Representation Learning\n",
    "- Deep networks internally build representations of patterns in the data.\n",
    "- Partially replace the need for feature engineering.\n",
    "- Subsequent layers build increasingly sophisticated representations of raw data.\n",
    "- First layer might detect edges, second layer might detect shapes, third layer might detect high-level features. It shows us how deep learning models can learn from the data.\n",
    "\n",
    "### Deep Learning\n",
    "- Modeler doesn't need to specify the interactions.\n",
    "- When you train the model, the neural network gets weights that find the relevant patterns to make better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The need for optimization\n",
    "- Optimization finds the set of weights that minimizes the loss function.\n",
    "- Loss function measures how well the model's predictions match the target values.\n",
    "- We use the data to update the weights.\n",
    "- Making accurate predictions gets harder with more points.\n",
    "- At any set of weights, there are many values of the error corresponding to the many points we make predictions for.\n",
    "\n",
    "### Loss Function\n",
    "- Aggregates errors in predictions from many data points into single number.\n",
    "- Measure of model's predictive performance.\n",
    "- Lower loss function value means a better model.\n",
    "- We use the mean squared error loss function.\n",
    "- [Loss Function Graph](loss_function_graph.png)\n",
    "- Goal: Find the weights that give the lowest value for the loss function.\n",
    "- Gradient descent is a general method to minimize functions.\n",
    "\n",
    "### Gradient Descent\n",
    "- Imagine you are in a pitch dark field.\n",
    "- Want to find the lowest point.\n",
    "- Feel the ground to see how to go downhill.\n",
    "- Take small steps downhill.\n",
    "- Repeat until it is uphill in every direction.\n",
    "- This is gradient descent.\n",
    "- Steps:\n",
    "    - Start at random point.\n",
    "    - Until you are somewhere flat:\n",
    "        - Find the slope.\n",
    "        - Take a step downhill.\n",
    "- Learning rate: how big the step is.\n",
    "- Too big: might miss the minimum.\n",
    "- Too small: will take too long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
