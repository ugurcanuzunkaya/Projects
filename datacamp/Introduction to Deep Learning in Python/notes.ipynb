{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning in Python Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Deep Learning\n",
    "- Deep learning is a subset of machine learning, which is a subset of artificial intelligence.\n",
    "- Deep learning is a type of machine learning that trains a computer to perform human-like tasks, such as recognizing speech, identifying images or making predictions.\n",
    "- Instead of organizing data to run through predefined equations, deep learning sets up basic parameters about the data and trains the computer to learn on its own by recognizing patterns using many layers of processing.\n",
    "- We use Keras for this course. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "- Forward propagation is the process neural networks use to make predictions.\n",
    "- Bank transactions example:\n",
    "    - Inputs: number of children, number of existing accounts.\n",
    "    - Output: number of transactions.\n",
    "    - Weights: parameters that the model learns.\n",
    "    - [Model Photo](forward_propagation.png)\n",
    "- Dot product: multiply the inputs by the weights and sum them up.\n",
    "- Forward propagation for one data point. Output is the prediction for that data point.\n",
    "- Code:\n",
    "```python\n",
    "import numpy as np\n",
    "input_data = np.array([2, 3])\n",
    "weights = {'node_0': np.array([1, 1]),\n",
    "           'node_1': np.array([-1, 1]),\n",
    "           'output': np.array([2, -1])}\n",
    "node_0_value = (input_data * weights['node_0']).sum()\n",
    "node_1_value = (input_data * weights['node_1']).sum()\n",
    "hidden_layer_values = np.array([node_0_value, node_1_value])\n",
    "print(hidden_layer_values)\n",
    "output = (hidden_layer_values * weights['output']).sum()\n",
    "print(output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "- Activation functions are applied to node inputs to produce node output.\n",
    "- An activation function allows models to capture non-linearities.\n",
    "- If the relationships in the data are non-linear, we need an activation function to capture them.\n",
    "- Applied to node inputs, activation functions produce node outputs.\n",
    "- Activation functions:\n",
    "    - ReLU (Rectified Linear Activation): max(0, x)\n",
    "    - Tanh (Hyperbolic Tangent): (e^x - e^-x) / (e^x + e^-x)\n",
    "    - Sigmoid: 1 / (1 + e^-x)\n",
    "    - Identity: f(x) = x\n",
    "- [Activation Function Tanh](activation_function_tanh.png)\n",
    "\n",
    "### ReLU (Rectified Linear Activation)\n",
    "- ReLU is the most common activation function.\n",
    "- [Activation Function ReLU](ReLU.png)\n",
    "\n",
    "### Tanh Example\n",
    "- Example code:\n",
    "```python\n",
    "import numpy as np\n",
    "input_data = np.array([-1, 2])\n",
    "weights = {'node_0': np.array([3, 3]),\n",
    "           'node_1': np.array([1, 5]),\n",
    "           'output': np.array([2, -1])}\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = np.tanh(node_0_input)\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = np.tanh(node_1_input)\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "output = (hidden_layer_outputs * weights['output']).sum()\n",
    "print(output)\n",
    "```\n",
    "\n",
    "### ReLU Example\n",
    "- Example code:\n",
    "```python\n",
    "def relu(input):\n",
    "    '''Define your relu activation function here'''\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(0, input)\n",
    "    \n",
    "    # Return the value just calculated\n",
    "    return(output)\n",
    "\n",
    "# Calculate node 0 value: node_0_output\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = relu(node_0_input)\n",
    "\n",
    "# Calculate node 1 value: node_1_output\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = relu(node_1_input)\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "# Calculate model output (do not apply relu)\n",
    "model_output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print model output\n",
    "print(model_output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper Networks\n",
    "- There is more than one hidden layer. \n",
    "- Deep networks internally build representations of patterns in the data.\n",
    "- You use same forward propagation process, but apply it multiple times iteratively.\n",
    "- [Deeper Networks](multiple_hidden_layers_relu.png)\n",
    "- This is the mechanics for how neural networks make predictions.\n",
    "\n",
    "### Representation Learning\n",
    "- Deep networks internally build representations of patterns in the data.\n",
    "- Partially replace the need for feature engineering.\n",
    "- Subsequent layers build increasingly sophisticated representations of raw data.\n",
    "- First layer might detect edges, second layer might detect shapes, third layer might detect high-level features. It shows us how deep learning models can learn from the data.\n",
    "\n",
    "### Deep Learning\n",
    "- Modeler doesn't need to specify the interactions.\n",
    "- When you train the model, the neural network gets weights that find the relevant patterns to make better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The need for optimization\n",
    "- Optimization finds the set of weights that minimizes the loss function.\n",
    "- Loss function measures how well the model's predictions match the target values.\n",
    "- We use the data to update the weights.\n",
    "- Making accurate predictions gets harder with more points.\n",
    "- At any set of weights, there are many values of the error corresponding to the many points we make predictions for.\n",
    "\n",
    "### Loss Function\n",
    "- Aggregates errors in predictions from many data points into single number.\n",
    "- Measure of model's predictive performance.\n",
    "- Lower loss function value means a better model.\n",
    "- We use the mean squared error loss function.\n",
    "- [Loss Function Graph](loss_function_graph.png)\n",
    "- Goal: Find the weights that give the lowest value for the loss function.\n",
    "- Gradient descent is a general method to minimize functions.\n",
    "\n",
    "### Gradient Descent\n",
    "- Imagine you are in a pitch dark field.\n",
    "- Want to find the lowest point.\n",
    "- Feel the ground to see how to go downhill.\n",
    "- Take small steps downhill.\n",
    "- Repeat until it is uphill in every direction.\n",
    "- This is gradient descent.\n",
    "- Steps:\n",
    "    - Start at random point.\n",
    "    - Until you are somewhere flat:\n",
    "        - Find the slope.\n",
    "        - Take a step downhill.\n",
    "- Learning rate: how big the step is.\n",
    "- Too big: might miss the minimum.\n",
    "- Too small: will take too long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "- The importance of model weights in making accurate predictions. Adjusting weights can significantly change the model's output.\n",
    "- The concept of a loss function, which aggregates all prediction errors into a single measure, helping to evaluate the model's performance.\n",
    "- Gradient descent, an algorithm used to find the set of weights that minimizes the loss function. It involves starting with random weights, calculating the slope (or gradient) of the loss function at those weights, and then adjusting the weights in the direction that reduces the loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "- If the slope is positive:\n",
    "    - Going opposite the slope means moving to lower numbers.\n",
    "    - Subtract the slope from the current value.\n",
    "    - Too big a step might lead us astray.\n",
    "- Learning rate: how much we update the weights. Update each weight by subtracting the product of learning rate and slope.\n",
    "- Slope calculation for a weight, need to multiply:\n",
    "    - Slope of the loss function w.r.t (with respect to) the value at the node we feed into.\n",
    "    - The value of the node that feeds into our weight.\n",
    "    - Slope of the activation function w.r.t the value we feed into.\n",
    "- Slope of mean-squared loss function w.r.t prediction:\n",
    "    - 2 * (prediction - actual) = 2 * error.\n",
    "\n",
    "### Gradient Descent Example\n",
    "[Example](slope_calculation_example.png)\n",
    "- 2 * -4 = -8 (slope of the loss function w.r.t prediction).\n",
    "- 2 * -4 * 3 = -24 (slope of the loss function w.r.t prediction * node value).\n",
    "- If learning rate is 0.01, the new weight would be 2 - 0.01 * -24 = 2.24. This is how we update the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "- Takes the error from the output layer and propagates it backward through the network.\n",
    "- It calculates the necessary slopes sequentially from the weights closest to the prediction, through the hidden layers, to the input layer.\n",
    "- Allow gradient descent to update all weights in the neural network (by getting gradients for all weights).\n",
    "- Comes from chain rule of calculus.\n",
    "- Process:\n",
    "    - Trying to estimate the slope of the loss function w.r.t each weight.\n",
    "    - Do forward propagation to calculate predictions and errors.\n",
    "    - Go back one layer at a time.\n",
    "    - Gradients for weight is product of:\n",
    "        - Node value feeding into that weight.\n",
    "        - Slope of loss function w.r.t node it feeds into.\n",
    "        - Slope of activation function at the node it feeds into.\n",
    "    - Use these gradients to update the weights.\n",
    "    - Need to also keep track of slopes of the loss function w.r.t node values.\n",
    "    - Slope of node values are the sum of the slopes for all weights that come out of them.\n",
    "\n",
    "### ReLU Activation Function\n",
    "- Slope is 0 for negative values.\n",
    "- Slope is 1 for positive values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation In Practice\n",
    "- Calculating slopes associated with any weight in the network.\n",
    "- Gradients for weight is product of:\n",
    "    - Node value feeding into that weight.\n",
    "    - Slope of the loss function w.r.t node it feeds into.\n",
    "    - Slope of activation function at the node it feeds into.\n",
    "\n",
    "[Backpropagation](backpropagation_example.png)\n",
    "\n",
    "### Recap\n",
    "- Start at some random set of weights.\n",
    "- Use forward propagation to make a prediction.\n",
    "- Use backward propagation to calculate the slope of the loss function w.r.t each weight.\n",
    "- Multiply that slope by the learning rate, and subtract from the current weights.\n",
    "- Keep going with that cycle until we get to a flat part.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "- It is common to calculate slopes on only a subset of the data ('batch').\n",
    "- Use a different batch of data to calculate the next update.\n",
    "- Start over from the beginning once all data is used.\n",
    "- Each time through the training data is called an epoch.\n",
    "- When slopes are calculated on one batch at a time: stochastic gradient descent.\n",
    "- When slopes are calculated on the whole data set: gradient descent.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
