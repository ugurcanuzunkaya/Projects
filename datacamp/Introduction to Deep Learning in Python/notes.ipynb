{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning in Python Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Deep Learning\n",
    "- Deep learning is a subset of machine learning, which is a subset of artificial intelligence.\n",
    "- Deep learning is a type of machine learning that trains a computer to perform human-like tasks, such as recognizing speech, identifying images or making predictions.\n",
    "- Instead of organizing data to run through predefined equations, deep learning sets up basic parameters about the data and trains the computer to learn on its own by recognizing patterns using many layers of processing.\n",
    "- We use Keras for this course. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "- Forward propagation is the process neural networks use to make predictions.\n",
    "- Bank transactions example:\n",
    "    - Inputs: number of children, number of existing accounts.\n",
    "    - Output: number of transactions.\n",
    "    - Weights: parameters that the model learns.\n",
    "    - [Model Photo](forward_propagation.png)\n",
    "- Dot product: multiply the inputs by the weights and sum them up.\n",
    "- Forward propagation for one data point. Output is the prediction for that data point.\n",
    "- Code:\n",
    "```python\n",
    "import numpy as np\n",
    "input_data = np.array([2, 3])\n",
    "weights = {'node_0': np.array([1, 1]),\n",
    "           'node_1': np.array([-1, 1]),\n",
    "           'output': np.array([2, -1])}\n",
    "node_0_value = (input_data * weights['node_0']).sum()\n",
    "node_1_value = (input_data * weights['node_1']).sum()\n",
    "hidden_layer_values = np.array([node_0_value, node_1_value])\n",
    "print(hidden_layer_values)\n",
    "output = (hidden_layer_values * weights['output']).sum()\n",
    "print(output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "- Activation functions are applied to node inputs to produce node output.\n",
    "- An activation function allows models to capture non-linearities.\n",
    "- If the relationships in the data are non-linear, we need an activation function to capture them.\n",
    "- Applied to node inputs, activation functions produce node outputs.\n",
    "- Activation functions:\n",
    "    - ReLU (Rectified Linear Activation): max(0, x)\n",
    "    - Tanh (Hyperbolic Tangent): (e^x - e^-x) / (e^x + e^-x)\n",
    "    - Sigmoid: 1 / (1 + e^-x)\n",
    "    - Identity: f(x) = x\n",
    "- [Activation Function Tanh](activation_function_tanh.png)\n",
    "\n",
    "### ReLU (Rectified Linear Activation)\n",
    "- ReLU is the most common activation function.\n",
    "- [Activation Function ReLU](ReLU.png)\n",
    "\n",
    "### Tanh Example\n",
    "- Example code:\n",
    "```python\n",
    "import numpy as np\n",
    "input_data = np.array([-1, 2])\n",
    "weights = {'node_0': np.array([3, 3]),\n",
    "           'node_1': np.array([1, 5]),\n",
    "           'output': np.array([2, -1])}\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = np.tanh(node_0_input)\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = np.tanh(node_1_input)\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "output = (hidden_layer_outputs * weights['output']).sum()\n",
    "print(output)\n",
    "```\n",
    "\n",
    "### ReLU Example\n",
    "- Example code:\n",
    "```python\n",
    "def relu(input):\n",
    "    '''Define your relu activation function here'''\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(0, input)\n",
    "    \n",
    "    # Return the value just calculated\n",
    "    return(output)\n",
    "\n",
    "# Calculate node 0 value: node_0_output\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = relu(node_0_input)\n",
    "\n",
    "# Calculate node 1 value: node_1_output\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = relu(node_1_input)\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "# Calculate model output (do not apply relu)\n",
    "model_output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print model output\n",
    "print(model_output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper Networks\n",
    "- There is more than one hidden layer. \n",
    "- Deep networks internally build representations of patterns in the data.\n",
    "- You use same forward propagation process, but apply it multiple times iteratively.\n",
    "- [Deeper Networks](multiple_hidden_layers_relu.png)\n",
    "- This is the mechanics for how neural networks make predictions.\n",
    "\n",
    "### Representation Learning\n",
    "- Deep networks internally build representations of patterns in the data.\n",
    "- Partially replace the need for feature engineering.\n",
    "- Subsequent layers build increasingly sophisticated representations of raw data.\n",
    "- First layer might detect edges, second layer might detect shapes, third layer might detect high-level features. It shows us how deep learning models can learn from the data.\n",
    "\n",
    "### Deep Learning\n",
    "- Modeler doesn't need to specify the interactions.\n",
    "- When you train the model, the neural network gets weights that find the relevant patterns to make better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The need for optimization\n",
    "- Optimization finds the set of weights that minimizes the loss function.\n",
    "- Loss function measures how well the model's predictions match the target values.\n",
    "- We use the data to update the weights.\n",
    "- Making accurate predictions gets harder with more points.\n",
    "- At any set of weights, there are many values of the error corresponding to the many points we make predictions for.\n",
    "\n",
    "### Loss Function\n",
    "- Aggregates errors in predictions from many data points into single number.\n",
    "- Measure of model's predictive performance.\n",
    "- Lower loss function value means a better model.\n",
    "- We use the mean squared error loss function.\n",
    "- [Loss Function Graph](loss_function_graph.png)\n",
    "- Goal: Find the weights that give the lowest value for the loss function.\n",
    "- Gradient descent is a general method to minimize functions.\n",
    "\n",
    "### Gradient Descent\n",
    "- Imagine you are in a pitch dark field.\n",
    "- Want to find the lowest point.\n",
    "- Feel the ground to see how to go downhill.\n",
    "- Take small steps downhill.\n",
    "- Repeat until it is uphill in every direction.\n",
    "- This is gradient descent.\n",
    "- Steps:\n",
    "    - Start at random point.\n",
    "    - Until you are somewhere flat:\n",
    "        - Find the slope.\n",
    "        - Take a step downhill.\n",
    "- Learning rate: how big the step is.\n",
    "- Too big: might miss the minimum.\n",
    "- Too small: will take too long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "- The importance of model weights in making accurate predictions. Adjusting weights can significantly change the model's output.\n",
    "- The concept of a loss function, which aggregates all prediction errors into a single measure, helping to evaluate the model's performance.\n",
    "- Gradient descent, an algorithm used to find the set of weights that minimizes the loss function. It involves starting with random weights, calculating the slope (or gradient) of the loss function at those weights, and then adjusting the weights in the direction that reduces the loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "- If the slope is positive:\n",
    "    - Going opposite the slope means moving to lower numbers.\n",
    "    - Subtract the slope from the current value.\n",
    "    - Too big a step might lead us astray.\n",
    "- Learning rate: how much we update the weights. Update each weight by subtracting the product of learning rate and slope.\n",
    "- Slope calculation for a weight, need to multiply:\n",
    "    - Slope of the loss function w.r.t (with respect to) the value at the node we feed into.\n",
    "    - The value of the node that feeds into our weight.\n",
    "    - Slope of the activation function w.r.t the value we feed into.\n",
    "- Slope of mean-squared loss function w.r.t prediction:\n",
    "    - 2 * (prediction - actual) = 2 * error.\n",
    "\n",
    "### Gradient Descent Example\n",
    "[Example](slope_calculation_example.png)\n",
    "- 2 * -4 = -8 (slope of the loss function w.r.t prediction).\n",
    "- 2 * -4 * 3 = -24 (slope of the loss function w.r.t prediction * node value).\n",
    "- If learning rate is 0.01, the new weight would be 2 - 0.01 * -24 = 2.24. This is how we update the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "- Takes the error from the output layer and propagates it backward through the network.\n",
    "- It calculates the necessary slopes sequentially from the weights closest to the prediction, through the hidden layers, to the input layer.\n",
    "- Allow gradient descent to update all weights in the neural network (by getting gradients for all weights).\n",
    "- Comes from chain rule of calculus.\n",
    "- Process:\n",
    "    - Trying to estimate the slope of the loss function w.r.t each weight.\n",
    "    - Do forward propagation to calculate predictions and errors.\n",
    "    - Go back one layer at a time.\n",
    "    - Gradients for weight is product of:\n",
    "        - Node value feeding into that weight.\n",
    "        - Slope of loss function w.r.t node it feeds into.\n",
    "        - Slope of activation function at the node it feeds into.\n",
    "    - Use these gradients to update the weights.\n",
    "    - Need to also keep track of slopes of the loss function w.r.t node values.\n",
    "    - Slope of node values are the sum of the slopes for all weights that come out of them.\n",
    "\n",
    "### ReLU Activation Function\n",
    "- Slope is 0 for negative values.\n",
    "- Slope is 1 for positive values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation In Practice\n",
    "- Calculating slopes associated with any weight in the network.\n",
    "- Gradients for weight is product of:\n",
    "    - Node value feeding into that weight.\n",
    "    - Slope of the loss function w.r.t node it feeds into.\n",
    "    - Slope of activation function at the node it feeds into.\n",
    "\n",
    "[Backpropagation](backpropagation_example.png)\n",
    "\n",
    "### Recap\n",
    "- Start at some random set of weights.\n",
    "- Use forward propagation to make a prediction.\n",
    "- Use backward propagation to calculate the slope of the loss function w.r.t each weight.\n",
    "- Multiply that slope by the learning rate, and subtract from the current weights.\n",
    "- Keep going with that cycle until we get to a flat part.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "- It is common to calculate slopes on only a subset of the data ('batch').\n",
    "- Use a different batch of data to calculate the next update.\n",
    "- Start over from the beginning once all data is used.\n",
    "- Each time through the training data is called an epoch.\n",
    "- When slopes are calculated on one batch at a time: stochastic gradient descent.\n",
    "- When slopes are calculated on the whole data set: gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Keras Model\n",
    "- Model building steps:\n",
    "    - Specify architecture.\n",
    "        - How many layers?\n",
    "        - How many nodes in each layer?\n",
    "        - What activation function?\n",
    "    - Compile.\n",
    "        - Loss function.\n",
    "        - Optimizer.\n",
    "    - Fit.\n",
    "        - Iteratively improve weights.\n",
    "        - Backpropagation.\n",
    "        - Optimization of weights.\n",
    "    - Predict.\n",
    "\n",
    "### Model Specification\n",
    "```python\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "predictors = np.loadtxt('predictors_data.csv', delimiter=',')\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "```\n",
    "- Sequential model: a linear stack of layers. You can create a Sequential model by passing a list of layer instances to the constructor.\n",
    "- Dense: a regular densely-connected NN layer. It is the most common and frequently used layer.\n",
    "- Activation: the activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "- Input_shape: the shape of the input to the model. It is required when using this layer as the first layer in a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling and fitting a model\n",
    "- Why you need to compile your model:\n",
    "    - Specify the optimizer.\n",
    "        - Controls the learning rate.\n",
    "        - Many options and mathematically complex.\n",
    "        - \"Adam\" is usually a good choice.\n",
    "    - Loss function.\n",
    "        - \"mean_squared_error\" common for regression.\n",
    "        - \"categorical_crossentropy\" common for classification.\n",
    "    - Metrics.\n",
    "\n",
    "### Compiling a model\n",
    "```python\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "```\n",
    "\n",
    "### Fitting a model\n",
    "- What is fitting a model?\n",
    "    - Applying backpropagation and gradient descent with your data to update the weights.\n",
    "    - Scaling data before fitting can ease optimization.\n",
    "```python\n",
    "model.fit(predictors, target)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models\n",
    "- Classification: 'categorical_crossentropy' loss function.\n",
    "- Similar to log loss: lower is better.\n",
    "- Add metrics = ['accuracy'] to compile step for easy-to-understand diagnostics.\n",
    "- Output layer has separate node for each possible outcome, and uses 'softmax' activation.\n",
    "- Softmax ensures the predictions sum to 1 so they can be interpreted as probabilities.\n",
    "\n",
    "### Classification Example\n",
    "```python\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "data = pd.read_csv('basketball_shot_log.csv')\n",
    "predictors = data.drop(['shot_result'], axis=1).as_matrix()\n",
    "target = to_categorical(data.shot_result)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(predictors, target)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Models\n",
    "- Save model after fitting.\n",
    "- Reload model.\n",
    "- Make predictions.\n",
    "\n",
    "### Saving, reloading and using your Model\n",
    "```python\n",
    "from tensorflow.keras.models import load_model\n",
    "model.save('model_file.h5')\n",
    "my_model = load_model('my_model.h5')\n",
    "predictions = my_model.predict(data_to_predict_with)\n",
    "probability_true = predictions[:,1]\n",
    "```\n",
    "\n",
    "```python\n",
    "my_model.summary()\n",
    "```\n",
    "- Summary method shows the model's architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand Model Optimization\n",
    "- Why optimization is hard:\n",
    "    - Simultanously update thousands of weights with complex relationships.\n",
    "    - Updates may not improve model meaningfully.\n",
    "    - Updates too small (if learning rate is low) or too large (if learning rate is high).\n",
    "    - Getting stuck in local minima.\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "- Stochastic gradient descent:\n",
    "    - Uses only a random subset of the data for each batch.\n",
    "    - Faster than standard gradient descent.\n",
    "    - Can be less stable than standard gradient descent.\n",
    "    - Keras has the 'SGD' optimizer.\n",
    "- Example:\n",
    "```python\n",
    "def get_new_model(input_shape = input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    return(model)\n",
    "\n",
    "lr_to_test = [0.000001, 0.01, 1]\n",
    "\n",
    "# Loop over learning rates\n",
    "for lr in lr_to_test:\n",
    "    model = get_new_model()\n",
    "    my_optimizer = SGD(lr=lr)\n",
    "    model.compile(optimizer=my_optimizer, loss='categorical_crossentropy')\n",
    "    model.fit(predictors, target)\n",
    "```\n",
    "\n",
    "### Dying Neuron Problem\n",
    "- This problem occurs when a neuron takes a value less than 0 for all rows of data.\n",
    "- With the ReLU activation function, a negative input will output 0 and it also has a slope of 0.\n",
    "- This means that the weights will not be updated. The neuron is \"dead\" because it is unlikely to fire for any data point.\n",
    "\n",
    "### Vanishing Gradients\n",
    "- Occurs when many layers have very small slopes (e.g. due to being on flat part of tanh curve).\n",
    "- In deep networks, updates to backprop were close to 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation\n",
    "- Validation in deep learning:\n",
    "    - Commonly use validation split rather than cross-validation due to computational expense.\n",
    "    - Deep learning widely used on large datasets, training on a single split is common.\n",
    "    - Single validation score is based on large amount of data, and is reliable.\n",
    "```python\n",
    "model.fit(predictors, target, validation_split=0.3)\n",
    "```\n",
    "\n",
    "### Early Stopping\n",
    "- Early stopping:\n",
    "    - End training when performance on validation set starts to degrade.\n",
    "    - Avoid overfitting.\n",
    "    - Keras has 'EarlyStopping' callback for this.\n",
    "    - 2 or 3 are reasonable numbers of epochs to wait.\n",
    "    - Patience parameter: number of epochs to wait before stopping.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "model.fit(predictors, target, validation_split=0.3, epochs=20, callbacks=[early_stopping_monitor])\n",
    "```\n",
    "\n",
    "### Experimentation\n",
    "- Experiment with different architectures.\n",
    "- More layers.\n",
    "- Fewer layers.\n",
    "- Layers with more nodes.\n",
    "- Layers with fewer nodes.\n",
    "- Early stopping.\n",
    "- Creating a great model requires experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking About Model Capacity\n",
    "- Model capacity or network capacity:\n",
    "    - Model's ability to capture predictive patterns in data.\n",
    "    - Model capacity too low: underfitting.\n",
    "    - Model capacity too high: overfitting. [Overfitting](overfittiing.png)\n",
    "- Overfitting\n",
    "    - Overfitting: model memorizes training data rather than learning from it.\n",
    "    - Overfitting is the ability of a model to fit oddities in your training data that are there purely due to happenstance, and that won't apply in an new dataset.\n",
    "    - When you are overfitting, your model will make accurate predictions on the training data, but it will make inaccurate predictions on validation data and new datatests.\n",
    "- Underfitting\n",
    "    - Underfitting: model is not able to capture the patterns in the data. Model fails to find important predictions in the training data.\n",
    "    - Underfitting is when the model is too simple to learn the underlying structure of the data.\n",
    "    - When you are underfitting, your model will make inaccurate predictions on both the training data and the validation data.\n",
    "- Our validation score is the key indicator of how well our model is generalizing to new data. Ultimate measure of a model's predictive quality.\n",
    "- Model capacity is a model's ability to capture predictive patterns in data.\n",
    "- So, the more capacity a model has, the more patterns it can capture. Making larger layers or adding more layers increases the model's capacity.\n",
    "\n",
    "### Worflow for Optimizing Model Capacity\n",
    "- Start with a small network.\n",
    "- Gradually increase capacity.\n",
    "- Keep increasing capacity until validation score is no longer improving.\n",
    "- [Model Capacity](sequential_experiments_model_capacity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepping Up To Images\n",
    "### Recognizing handwritten digits\n",
    "- MNIST dataset: images of handwritten digits.\n",
    "- 28 x 28 grid flattened to 784 values for each image.\n",
    "- Value in each part of array represents darkness of that pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "- Deep learning is widely used and rapidly changing field.\n",
    "- Keras is a great tool for building deep learning models.\n",
    "- There are many techniques to improve your model's performance.\n",
    "- Experiment with different architectures to improve performance.\n",
    "\n",
    "###Â Next Steps\n",
    "- Start with standard prediction problems on tables of numbers.\n",
    "- Images (with convolutional neural networks) are a common next step.\n",
    "- Text data (with recurrent neural networks) is another common next step.\n",
    "- Kaggle is a great place to practice.\n",
    "- Wikipedia and deeplearning.ai are great resources.\n",
    "- List of datasets for machine learning research: https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research\n",
    "- List of datasets for computer vision and image processing: https://en.wikipedia.org/wiki/List_of_datasets_in_computer_vision_and_image_processing\n",
    "- keras.io for excellent documentation.\n",
    "- Graphical Processing Units (GPUs) can speed up training.\n",
    "- Need a CUDA-compatible Nvidia GPU.\n",
    "- For training on a GPU in the cloud, consider using Amazon Web Services, Google Cloud Platform, or Microsoft Azure."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
